/*
 * Live Translate WebSocket Service
 * åŸºäº qwen3-livetranslate-flash-realtime çš„å®æ—¶ç¿»è¯‘æœåŠ¡
 */

import Foundation
import UIKit
import AVFoundation

// MARK: - Service Class

class LiveTranslateService: NSObject {

    // WebSocket
    private var webSocket: URLSessionWebSocketTask?
    private var urlSession: URLSession?

    // Configuration
    private let apiKey: String
    private let model = "qwen3-livetranslate-flash-realtime"
    // æ ¹æ®ç”¨æˆ·è®¾ç½®çš„åŒºåŸŸåŠ¨æ€è·å– WebSocket URL
    private var baseURL: String {
        return APIProviderManager.staticLiveAIWebsocketURL
    }

    // Audio Engine (for recording)
    private var audioEngine: AVAudioEngine?

    // Audio Playback Engine (separate engine for playback)
    private var playbackEngine: AVAudioEngine?
    private var playerNode: AVAudioPlayerNode?
    private let playbackFormat = AVAudioFormat(standardFormatWithSampleRate: 24000, channels: 1)

    // Audio buffer management
    private var audioBuffer = Data()
    private var isCollectingAudio = false
    private var audioChunkCount = 0
    private let minChunksBeforePlay = 2
    private var hasStartedPlaying = false
    private var isPlaybackEngineRunning = false

    // Translation settings
    private var sourceLanguage: TranslateLanguage = .en
    private var targetLanguage: TranslateLanguage = .zh
    private var voice: TranslateVoice = .cherry
    private var audioOutputEnabled = true

    // Audio resampling
    private var audioConverter: AVAudioConverter?
    private let targetSampleRate: Double = 16000  // API expects 16kHz

    // Callbacks
    var onConnected: (() -> Void)?
    var onTranslationText: ((String) -> Void)?    // ç¿»è¯‘ç»“æœæ–‡æœ¬
    var onTranslationDelta: ((String) -> Void)?   // å¢é‡ç¿»è¯‘æ–‡æœ¬
    var onAudioDelta: ((Data) -> Void)?
    var onAudioDone: (() -> Void)?
    var onError: ((String) -> Void)?

    // State
    private var isRecording = false
    private var eventIdCounter = 0

    // Image sending
    private var lastImageSendTime: Date?
    private let imageInterval: TimeInterval = 0.5  // æ¯0.5ç§’æœ€å¤šå‘é€ä¸€å¼ å›¾ç‰‡

    init(apiKey: String) {
        self.apiKey = apiKey
        super.init()
        setupAudioEngine()
    }

    // MARK: - Audio Engine Setup

    private func setupAudioEngine() {
        audioEngine = AVAudioEngine()
        setupPlaybackEngine()
    }

    private func setupPlaybackEngine() {
        playbackEngine = AVAudioEngine()
        playerNode = AVAudioPlayerNode()

        guard let playbackEngine = playbackEngine,
              let playerNode = playerNode,
              let playbackFormat = playbackFormat else {
            print("âŒ [Translate] æ— æ³•åˆå§‹åŒ–æ’­æ”¾å¼•æ“")
            return
        }

        playbackEngine.attach(playerNode)
        playbackEngine.connect(playerNode, to: playbackEngine.mainMixerNode, format: playbackFormat)
        playbackEngine.prepare()

        print("âœ… [Translate] æ’­æ”¾å¼•æ“åˆå§‹åŒ–å®Œæˆ: Float32 @ 24kHz")
    }

    private func startPlaybackEngine() {
        guard let playbackEngine = playbackEngine, !isPlaybackEngineRunning else { return }

        do {
            try playbackEngine.start()
            isPlaybackEngineRunning = true
            print("â–¶ï¸ [Translate] æ’­æ”¾å¼•æ“å·²å¯åŠ¨")
        } catch {
            print("âŒ [Translate] æ’­æ”¾å¼•æ“å¯åŠ¨å¤±è´¥: \(error)")
        }
    }

    private func stopPlaybackEngine() {
        guard let playbackEngine = playbackEngine, isPlaybackEngineRunning else { return }

        playerNode?.stop()
        playerNode?.reset()
        playbackEngine.stop()
        isPlaybackEngineRunning = false
        print("â¹ï¸ [Translate] æ’­æ”¾å¼•æ“å·²åœæ­¢")
    }

    // MARK: - WebSocket Connection

    func connect() {
        let urlString = "\(baseURL)?model=\(model)"
        print("ğŸ”Œ [Translate] å‡†å¤‡è¿æ¥ WebSocket: \(urlString)")

        guard let url = URL(string: urlString) else {
            print("âŒ [Translate] æ— æ•ˆçš„ URL")
            onError?("Invalid URL")
            return
        }

        var request = URLRequest(url: url)
        request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")

        let configuration = URLSessionConfiguration.default
        urlSession = URLSession(configuration: configuration, delegate: self, delegateQueue: OperationQueue())

        webSocket = urlSession?.webSocketTask(with: request)
        webSocket?.resume()

        print("ğŸ”Œ [Translate] WebSocket ä»»åŠ¡å·²å¯åŠ¨")
        receiveMessage()

        // ç­‰å¾…è¿æ¥åå‘é€é…ç½®
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            print("âš™ï¸ [Translate] å‡†å¤‡é…ç½®ä¼šè¯")
            self.configureSession()
        }
    }

    func disconnect() {
        print("ğŸ”Œ [Translate] æ–­å¼€ WebSocket è¿æ¥")
        webSocket?.cancel(with: .goingAway, reason: nil)
        webSocket = nil
        stopRecording()
        stopPlaybackEngine()
    }

    // MARK: - Configuration

    func updateSettings(
        sourceLanguage: TranslateLanguage,
        targetLanguage: TranslateLanguage,
        voice: TranslateVoice,
        audioEnabled: Bool
    ) {
        self.sourceLanguage = sourceLanguage
        self.targetLanguage = targetLanguage
        self.voice = voice
        self.audioOutputEnabled = audioEnabled

        // å¦‚æœå·²è¿æ¥ï¼Œé‡æ–°é…ç½®ä¼šè¯
        if webSocket != nil {
            configureSession()
        }
    }

    private func configureSession() {
        var modalities: [String] = ["text"]
        if audioOutputEnabled {
            modalities.append("audio")
        }

        let sessionConfig: [String: Any] = [
            "event_id": generateEventId(),
            "type": TranslateClientEvent.sessionUpdate.rawValue,
            "session": [
                "modalities": modalities,
                "voice": voice.rawValue,
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm24",
                "input_audio_transcription": [
                    "language": sourceLanguage.rawValue
                ],
                "translation": [
                    "language": targetLanguage.rawValue
                ],
                "turn_detection": [
                    "type": "server_vad",
                    "threshold": 0.5,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 500
                ]
            ]
        ]

        sendEvent(sessionConfig)
        print("ğŸ“¤ [Translate] é…ç½®ä¼šè¯: \(sourceLanguage.rawValue) â†’ \(targetLanguage.rawValue), éŸ³è‰²: \(voice.rawValue)")
    }

    // MARK: - Audio Recording

    func startRecording(usePhoneMic: Bool = false) {
        guard !isRecording else { return }

        do {
            print("ğŸ¤ [Translate] å¼€å§‹å½•éŸ³, ä½¿ç”¨\(usePhoneMic ? "iPhone" : "è“ç‰™")éº¦å…‹é£")

            if let engine = audioEngine, engine.isRunning {
                engine.stop()
                engine.inputNode.removeTap(onBus: 0)
            }

            let audioSession = AVAudioSession.sharedInstance()

            if usePhoneMic {
                // ä½¿ç”¨ iPhone éº¦å…‹é£ - é€‚åˆç¿»è¯‘å¯¹æ–¹è¯´çš„è¯
                try audioSession.setCategory(
                    .playAndRecord,
                    mode: .default,
                    options: [.defaultToSpeaker]  // ä¸å¯ç”¨è“ç‰™ï¼Œå¼ºåˆ¶ä½¿ç”¨ iPhone éº¦å…‹é£
                )
                print("ğŸ™ï¸ [Translate] ä½¿ç”¨ iPhone éº¦å…‹é£ï¼ˆç¿»è¯‘å¯¹æ–¹ï¼‰")
            } else {
                // ä½¿ç”¨è“ç‰™éº¦å…‹é£ï¼ˆçœ¼é•œï¼‰- é€‚åˆç¿»è¯‘è‡ªå·±è¯´çš„è¯
                try audioSession.setCategory(
                    .playAndRecord,
                    mode: .default,
                    options: [.allowBluetooth, .defaultToSpeaker]
                )
                print("ğŸ™ï¸ [Translate] ä½¿ç”¨è“ç‰™éº¦å…‹é£ï¼ˆç¿»è¯‘è‡ªå·±ï¼‰")
            }
            try audioSession.setActive(true)

            // æ‰“å°å½“å‰éŸ³é¢‘è¾“å…¥è®¾å¤‡
            if let inputRoute = audioSession.currentRoute.inputs.first {
                print("ğŸ™ï¸ [Translate] å½“å‰è¾“å…¥è®¾å¤‡: \(inputRoute.portName) (\(inputRoute.portType.rawValue))")
            }

            guard let engine = audioEngine else {
                print("âŒ [Translate] éŸ³é¢‘å¼•æ“æœªåˆå§‹åŒ–")
                return
            }

            let inputNode = engine.inputNode
            let inputFormat = inputNode.outputFormat(forBus: 0)

            print("ğŸµ [Translate] è¾“å…¥æ ¼å¼: \(inputFormat.sampleRate) Hz, \(inputFormat.channelCount) channels")
            print("ğŸµ [Translate] ç›®æ ‡æ ¼å¼: \(targetSampleRate) Hz (å°†è‡ªåŠ¨é‡é‡‡æ ·)")

            inputNode.installTap(onBus: 0, bufferSize: 4096, format: inputFormat) { [weak self] buffer, time in
                self?.processAudioBuffer(buffer)
            }

            engine.prepare()
            try engine.start()

            isRecording = true
            print("âœ… [Translate] å½•éŸ³å·²å¯åŠ¨")

        } catch {
            print("âŒ [Translate] å¯åŠ¨å½•éŸ³å¤±è´¥: \(error.localizedDescription)")
            onError?("Failed to start recording: \(error.localizedDescription)")
        }
    }

    func stopRecording() {
        guard isRecording else { return }

        print("ğŸ›‘ [Translate] åœæ­¢å½•éŸ³")
        audioEngine?.inputNode.removeTap(onBus: 0)
        audioEngine?.stop()
        isRecording = false
    }

    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        guard let floatChannelData = buffer.floatChannelData else { return }

        let inputSampleRate = buffer.format.sampleRate

        // å¦‚æœé‡‡æ ·ç‡ä¸æ˜¯ 16kHzï¼Œéœ€è¦é‡é‡‡æ ·
        if inputSampleRate != targetSampleRate {
            guard let resampledBuffer = resampleBuffer(buffer) else {
                return
            }
            sendBufferAsPCM16(resampledBuffer)
        } else {
            sendBufferAsPCM16(buffer)
        }
    }

    private func resampleBuffer(_ inputBuffer: AVAudioPCMBuffer) -> AVAudioPCMBuffer? {
        let inputFormat = inputBuffer.format
        guard let outputFormat = AVAudioFormat(standardFormatWithSampleRate: targetSampleRate, channels: 1) else {
            return nil
        }

        // åˆ›å»ºæˆ–æ›´æ–° converter
        if audioConverter == nil || audioConverter?.inputFormat != inputFormat {
            audioConverter = AVAudioConverter(from: inputFormat, to: outputFormat)
        }

        guard let converter = audioConverter else {
            print("âŒ [Translate] æ— æ³•åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨")
            return nil
        }

        // è®¡ç®—è¾“å‡ºå¸§æ•°
        let ratio = targetSampleRate / inputFormat.sampleRate
        let outputFrameCount = AVAudioFrameCount(Double(inputBuffer.frameLength) * ratio)

        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat, frameCapacity: outputFrameCount) else {
            return nil
        }

        var error: NSError?
        let inputBlock: AVAudioConverterInputBlock = { inNumPackets, outStatus in
            outStatus.pointee = .haveData
            return inputBuffer
        }

        converter.convert(to: outputBuffer, error: &error, withInputFrom: inputBlock)

        if let error = error {
            print("âŒ [Translate] é‡é‡‡æ ·å¤±è´¥: \(error.localizedDescription)")
            return nil
        }

        return outputBuffer
    }

    private func sendBufferAsPCM16(_ buffer: AVAudioPCMBuffer) {
        guard let floatChannelData = buffer.floatChannelData else { return }

        let frameLength = Int(buffer.frameLength)
        let channel = floatChannelData.pointee

        // Float32 â†’ PCM16
        var int16Data = [Int16](repeating: 0, count: frameLength)
        for i in 0..<frameLength {
            let sample = channel[i]
            let clampedSample = max(-1.0, min(1.0, sample))
            int16Data[i] = Int16(clampedSample * 32767.0)
        }

        let data = Data(bytes: int16Data, count: frameLength * MemoryLayout<Int16>.size)
        let base64Audio = data.base64EncodedString()

        sendAudioAppend(base64Audio)
    }

    // MARK: - Image Sending

    func sendImageFrame(_ image: UIImage) {
        // é™åˆ¶å‘é€é¢‘ç‡ï¼šæ¯0.5ç§’æœ€å¤šä¸€å¼ 
        let now = Date()
        if let lastTime = lastImageSendTime, now.timeIntervalSince(lastTime) < imageInterval {
            return
        }
        lastImageSendTime = now

        guard let imageData = image.jpegData(compressionQuality: 0.6) else {
            print("âŒ [Translate] æ— æ³•å‹ç¼©å›¾ç‰‡")
            return
        }

        // é™åˆ¶å›¾ç‰‡å¤§å° 500KB
        guard imageData.count <= 500 * 1024 else {
            print("âš ï¸ [Translate] å›¾ç‰‡è¿‡å¤§ï¼Œè·³è¿‡å‘é€")
            return
        }

        let base64Image = imageData.base64EncodedString()
        print("ğŸ“¸ [Translate] å‘é€å›¾ç‰‡: \(imageData.count) bytes")

        let event: [String: Any] = [
            "event_id": generateEventId(),
            "type": TranslateClientEvent.inputImageBufferAppend.rawValue,
            "image": base64Image
        ]
        sendEvent(event)
    }

    // MARK: - Send Events

    private func sendEvent(_ event: [String: Any]) {
        guard let jsonData = try? JSONSerialization.data(withJSONObject: event),
              let jsonString = String(data: jsonData, encoding: .utf8) else {
            print("âŒ [Translate] æ— æ³•åºåˆ—åŒ–äº‹ä»¶")
            return
        }

        let message = URLSessionWebSocketTask.Message.string(jsonString)
        webSocket?.send(message) { error in
            if let error = error {
                print("âŒ [Translate] å‘é€äº‹ä»¶å¤±è´¥: \(error.localizedDescription)")
                self.onError?("Send error: \(error.localizedDescription)")
            }
        }
    }

    private var audioSendCount = 0

    private func sendAudioAppend(_ base64Audio: String) {
        audioSendCount += 1
        if audioSendCount == 1 || audioSendCount % 50 == 0 {
            print("ğŸµ [Translate] å‘é€éŸ³é¢‘å— #\(audioSendCount), å¤§å°: \(base64Audio.count) bytes")
        }

        let event: [String: Any] = [
            "event_id": generateEventId(),
            "type": TranslateClientEvent.inputAudioBufferAppend.rawValue,
            "audio": base64Audio
        ]
        sendEvent(event)
    }

    // MARK: - Receive Messages

    private func receiveMessage() {
        webSocket?.receive { [weak self] result in
            switch result {
            case .success(let message):
                self?.handleMessage(message)
                self?.receiveMessage()

            case .failure(let error):
                print("âŒ [Translate] æ¥æ”¶æ¶ˆæ¯å¤±è´¥: \(error.localizedDescription)")
                self?.onError?("Receive error: \(error.localizedDescription)")
            }
        }
    }

    private func handleMessage(_ message: URLSessionWebSocketTask.Message) {
        switch message {
        case .string(let text):
            handleServerEvent(text)
        case .data(let data):
            if let text = String(data: data, encoding: .utf8) {
                handleServerEvent(text)
            }
        @unknown default:
            break
        }
    }

    private func handleServerEvent(_ jsonString: String) {
        guard let data = jsonString.data(using: .utf8),
              let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
              let type = json["type"] as? String else {
            print("âš ï¸ [Translate] æ”¶åˆ°æ— æ³•è§£æçš„æ¶ˆæ¯: \(jsonString.prefix(200))")
            return
        }

        // æ‰“å°æ‰€æœ‰æ”¶åˆ°çš„äº‹ä»¶ç±»å‹
        print("ğŸ“¥ [Translate] æ”¶åˆ°äº‹ä»¶: \(type)")

        DispatchQueue.main.async {
            switch type {
            case TranslateServerEvent.sessionCreated.rawValue,
                 TranslateServerEvent.sessionUpdated.rawValue:
                print("âœ… [Translate] ä¼šè¯å·²å»ºç«‹")
                self.onConnected?()

            case TranslateServerEvent.responseAudioTranscriptText.rawValue:
                // å¢é‡ç¿»è¯‘æ–‡æœ¬
                if let delta = json["delta"] as? String {
                    print("ğŸ’¬ [Translate] ç¿»è¯‘ç‰‡æ®µ: \(delta)")
                    self.onTranslationDelta?(delta)
                }

            case TranslateServerEvent.responseAudioTranscriptDone.rawValue:
                // ç¿»è¯‘æ–‡æœ¬å®Œæˆï¼ˆè¾“å‡ºéŸ³é¢‘+æ–‡æœ¬æ¨¡å¼ï¼‰
                if let text = json["text"] as? String {
                    print("âœ… [Translate] ç¿»è¯‘å®Œæˆ: \(text)")
                    self.onTranslationText?(text)
                }

            case TranslateServerEvent.responseTextDone.rawValue:
                // ç¿»è¯‘æ–‡æœ¬å®Œæˆï¼ˆä»…æ–‡æœ¬æ¨¡å¼ï¼‰
                if let text = json["text"] as? String {
                    print("âœ… [Translate] ç¿»è¯‘å®Œæˆ(æ–‡æœ¬): \(text)")
                    self.onTranslationText?(text)
                }

            case TranslateServerEvent.responseAudioDelta.rawValue:
                if let base64Audio = json["delta"] as? String,
                   let audioData = Data(base64Encoded: base64Audio) {
                    self.onAudioDelta?(audioData)
                    self.handleAudioChunk(audioData)
                }

            case TranslateServerEvent.responseAudioDone.rawValue:
                self.isCollectingAudio = false
                if !self.audioBuffer.isEmpty {
                    self.playAudio(self.audioBuffer)
                    self.audioBuffer = Data()
                }
                self.audioChunkCount = 0
                self.hasStartedPlaying = false
                self.onAudioDone?()

            case TranslateServerEvent.error.rawValue:
                if let error = json["error"] as? [String: Any],
                   let message = error["message"] as? String {
                    print("âŒ [Translate] æœåŠ¡å™¨é”™è¯¯: \(message)")
                    self.onError?(message)
                }

            default:
                break
            }
        }
    }

    // MARK: - Audio Playback

    private func handleAudioChunk(_ audioData: Data) {
        if !isCollectingAudio {
            isCollectingAudio = true
            audioBuffer = Data()
            audioChunkCount = 0
            hasStartedPlaying = false

            if isPlaybackEngineRunning {
                stopPlaybackEngine()
                setupPlaybackEngine()
                startPlaybackEngine()
                playerNode?.play()
            }
        }

        audioChunkCount += 1

        if !hasStartedPlaying {
            audioBuffer.append(audioData)
            if audioChunkCount >= minChunksBeforePlay {
                hasStartedPlaying = true
                playAudio(audioBuffer)
                audioBuffer = Data()
            }
        } else {
            playAudio(audioData)
        }
    }

    private func playAudio(_ audioData: Data) {
        guard let playerNode = playerNode,
              let playbackFormat = playbackFormat else { return }

        if !isPlaybackEngineRunning {
            startPlaybackEngine()
            playerNode.play()
        } else if !playerNode.isPlaying {
            playerNode.play()
        }

        guard let pcmBuffer = createPCMBuffer(from: audioData, format: playbackFormat) else { return }
        playerNode.scheduleBuffer(pcmBuffer)
    }

    private func createPCMBuffer(from data: Data, format: AVAudioFormat) -> AVAudioPCMBuffer? {
        let frameCount = data.count / 2
        guard frameCount > 0 else { return nil }

        guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: AVAudioFrameCount(frameCount)),
              let channelData = buffer.floatChannelData else { return nil }

        buffer.frameLength = AVAudioFrameCount(frameCount)

        // PCM16 â†’ Float32
        data.withUnsafeBytes { (bytes: UnsafeRawBufferPointer) in
            guard let baseAddress = bytes.baseAddress else { return }
            let int16Pointer = baseAddress.assumingMemoryBound(to: Int16.self)
            let floatData = channelData[0]
            for i in 0..<frameCount {
                floatData[i] = Float(int16Pointer[i]) / 32768.0
            }
        }

        return buffer
    }

    // MARK: - Helpers

    private func generateEventId() -> String {
        eventIdCounter += 1
        return "translate_\(eventIdCounter)_\(UUID().uuidString.prefix(8))"
    }
}

// MARK: - URLSessionWebSocketDelegate

extension LiveTranslateService: URLSessionWebSocketDelegate {
    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocol: String?) {
        print("âœ… [Translate] WebSocket è¿æ¥å·²å»ºç«‹")
    }

    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        let reasonString = reason.flatMap { String(data: $0, encoding: .utf8) } ?? "unknown"
        print("ğŸ”Œ [Translate] WebSocket å·²æ–­å¼€, closeCode: \(closeCode.rawValue), reason: \(reasonString)")
    }
}
