//
//  EmotionAnalysisManager.swift
//  CameraAccess
//
//  Created by zora on 2026/1/19.
//

import Foundation
import AVFoundation
import UIKit // ç”¨äºå›¾åƒå¤„ç†
import VideoToolbox
import OSLog
import MetricKit

private let performanceLog = OSLog(subsystem: "com.turbometa.emotion", category: "DataFlow")

// MARK: - æ ¸å¿ƒç®¡ç†å™¨
class EmotionAnalysisManager: NSObject, ObservableObject
{
    // åœ¨ EmotionAnalysisManager ç±»é¡¶éƒ¨å®šä¹‰
    // swift
    private let audioSendSemaphore = DispatchSemaphore(value: 1)
    private var audioChunkIndex = 0
    private let audioEngine = AVAudioEngine()
        
        // 1. å£°æ˜æŒä¹…åŒ–çš„è½¬æ¢å™¨ (use `audioConverter` below)
        
        // 2. ç›®æ ‡æ ¼å¼ï¼šå¿…é¡»ä¸ä½  Python åç«¯çš„ AAC_RATE ä¸€è‡´
    let aacSettings: [String: Any] = [
            AVFormatIDKey: kAudioFormatMPEG4AAC,
            AVSampleRateKey: 24000,
            AVNumberOfChannelsKey: 1,
            AVEncoderBitRateKey: 64000
        ]
    lazy var aacFormat: AVAudioFormat = {
            return AVAudioFormat(settings: aacSettings)!
        }()
    private var triggerRequestTimer: Timer?
    private let videoQueue = DispatchQueue(label: "com.emotion.videoSend", qos: .userInteractive)
    
    static let shared = EmotionAnalysisManager()
    private var heartbeatTimer: Timer?
    private var frameIndex = 0
    private var webSocketTask: URLSessionWebSocketTask?
    // Outgoing message queue (stores serialized JSON strings)
    private var outgoingQueue = [String]()
    private let outgoingQueueLock = DispatchQueue(label: "com.turbometa.outgoingQueue", qos: .userInitiated)
    private let outgoingWorkerQueue = DispatchQueue(label: "com.turbometa.outgoingWorker", qos: .utility)
    private let session = URLSession(configuration: .default)
    private var videoEncoder: H264Encoder?
    private var isRunning = false
    private var isAudioEnabled = false
    private var currentRequestId: String?
    private let userId = "11"
    private var audioConverter: AVAudioConverter? // åªåˆå§‹åŒ–ä¸€æ¬¡
    private let targetFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: 24000, channels: 1, interleaved: false)!
    
    private let processingQueue = DispatchQueue(label: "com.turbometa.emotion.processing", qos: .userInteractive)
    
    
    // Build a 7-byte ADTS header for an AAC frame
    private func adtsHeader(aacLength: Int, sampleRate: Int, channels: UInt8, profile: UInt8 = 2) -> Data {
        // ADTS header is 7 bytes long
        let adtsLength = aacLength + 7

        // Mapping common sample rates to ADTS freq index
        let freqIdxLookup: [Int: UInt8] = [96000:0, 88200:1, 64000:2, 48000:3, 44100:4, 32000:5, 24000:6, 22050:7, 16000:8, 12000:9, 11025:10, 8000:11, 7350:12]
        let freqIdx: UInt8 = freqIdxLookup[sampleRate] ?? 6 // default to 24000 if unknown

        var packet = [UInt8](repeating: 0, count: 7)
        packet[0] = 0xFF
        packet[1] = 0xF1 // syncword(12) + MPEG-4 + layer + protection_absent
        packet[2] = ((profile - 1) << 6) | (freqIdx << 2) | (channels >> 2)
        packet[3] = ((channels & 3) << 6) | UInt8((adtsLength >> 11) & 0x03)
        packet[4] = UInt8((adtsLength >> 3) & 0xFF)
        packet[5] = UInt8(((adtsLength & 0x7) << 5) | 0x1F)
        packet[6] = 0xFC
        return Data(packet)
    }

    private let wsURL: URL = {
            // æ£€æŸ¥å­—ç¬¦ä¸²é‡Œæœ‰æ²¡æœ‰ç©ºæ ¼æˆ–æ¢è¡Œ
            let token = "25942d659fd81c3a4faa8deae5d3e278.CwjYQzIEqF1uHX0f7EG9CiBfZN14qRimke4lixE9dzw"
            let urlStr = "wss://api.finnox.cn/gateway/v1/proxy/ws?token=\(token)"
//            let urlStr = "ws://10.10.40.232:8900/ws"
            
            if let url = URL(string: urlStr) {
                return url
            } else {
                print("âŒ ä¸¥é‡é”™è¯¯: URL æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥ Token æ˜¯å¦åŒ…å«ç©ºæ ¼")
                return URL(string: "wss://localhost")! // é˜²æ­¢å´©æºƒçš„ä¿åº•åœ°å€
            }
        }()

    
    @Published var isConnected: Bool = false {
        didSet {
            if isConnected {
                print("ğŸŒ WebSocket è¿æ¥æˆåŠŸï¼Œç­‰å¾… 1s æ‰§è¡Œåˆå§‹åŒ–...")
                
                // âœ… æ”¹è¿›ï¼šå¢åŠ é˜²æŠ–å»¶è¿Ÿï¼Œé¿å…é¢‘ç¹é‡è¿ä¿¡å·æŠ–åŠ¨
                DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                    guard self.isConnected else { return }
                    print("ğŸŒ WebSocket å·²è¿æ¥ï¼Œå¼€å§‹ 2 ç§’æ•°æ®é¢„çƒ­ï¼ˆä¸å‘æŒ‡ä»¤ï¼Œåªæ¨æµï¼‰...")
                    
                    // ç»™éŸ³è§†é¢‘ç¼–ç å’Œä¼ è¾“ç•™å‡ºæ—¶é—´ï¼Œç¡®ä¿åç«¯ç¼“å†²åŒºæœ‰æ•°æ®
                    DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
                        if self.isConnected {
                            print("ğŸš€ é¢„çƒ­ç»“æŸï¼Œå‘é€æ­£å¼åˆ†ææŒ‡ä»¤")
                            self.sendTriggerRequest()
                        }
                    }
                }
            }
        }
    }
    @Published var emotionResult: String = "ç­‰å¾…åˆ†æ..."
    @Published var emotionScores: [String: Double] = [:] // å­˜å‚¨ happy, sad ç­‰å…·ä½“æ•°å€¼
    @Published var aiReasoning: String = "æ­£åœ¨è§‚å¯Ÿå¾®è¡¨æƒ…ä¸è¯­è°ƒ..." // æ·±åº¦æ¨ç†
    @Published var aiAdvice: String = "æ­£åœ¨åˆ†æ..." // å»ºè®®
    @Published var dominantEmotion: String = "Neutral" // å½“å‰ä¸»å¯¼æƒ…ç»ª
    
    
    // è®¡æ•°å™¨
    
    // å¯åŠ¨åˆ†æ
    func start() {
        guard !isRunning else { return }
        print("[Emotion] ğŸš€ å¯åŠ¨åˆ†ææ¨¡å—")
        isRunning = true
        // âœ… ä¿®å¤ï¼šç«‹å³å¯ç”¨éŸ³é¢‘ï¼Œè€Œä¸æ˜¯ç­‰å¾… 15s å
        isAudioEnabled = true
        currentRequestId = nil
        connectWebSocket()
        startFakeVitalSigns()
        startTriggerRequestTimer() // å¯åŠ¨æ¯15ç§’å‘é€ä¸€æ¬¡è§¦å‘è¯·æ±‚
        checkMicrophonePermissionAndStartAudio() // æƒé™æ£€æŸ¥åå¯åŠ¨éŸ³é¢‘æ•è·
    }
    
    // åœæ­¢åˆ†æ
    func stop() {
        print("[Emotion] ğŸ›‘ åœæ­¢åˆ†ææ¨¡å—")
        isRunning = false
        isAudioEnabled = false
        currentRequestId = nil
        disconnectWebSocket()
        videoEncoder = nil
        stopTriggerRequestTimer() // åœæ­¢å®šæ—¶å™¨
        stopAudioCapture()
        
        // âœ… æ–°å¢ï¼šåœç”¨éŸ³é¢‘ä¼šè¯
        deactivateAudioSession()
    }
//    func startHeartbeat() {
//        // 1. åœæ­¢æ—§çš„è®¡æ—¶å™¨
//        heartbeatTimer?.invalidate()
//        
//        // 2. æ¯ 5 ç§’å‘é€ä¸€æ¬¡æç®€åŒ…
//        heartbeatTimer = Timer.scheduledTimer(withTimeInterval: 5.0, repeats: true) { [weak self] _ in
//            guard let self = self, self.isConnected else { return }
//            let payload: [String: Any] = ["timestamp": Date().timeIntervalSince1970]
//            self.sendMessage(type: "ping", payload: payload)
//            print("ğŸ“¡ [Heartbeat] Ping sent")
//        }
//    }
    // MARK: - éŸ³é¢‘ä¼šè¯é…ç½®
    private func configureAudioSession() {
         let audioSession = AVAudioSession.sharedInstance()
         do {
             // æ¨è playAndRecordï¼Œå…¼å®¹æ›´å¤šè®¾å¤‡
             try audioSession.setCategory(.playAndRecord, mode: .measurement, options: [.duckOthers, .defaultToSpeaker])
             try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
             // AudioSession configured and activated (log removed)
         } catch {
            // AudioSession configuration/activation failed (log removed): \(error.localizedDescription)
         }
     }

     private func deactivateAudioSession() {
         let audioSession = AVAudioSession.sharedInstance()
         do {
             try audioSession.setActive(false, options: .notifyOthersOnDeactivation)
            // AudioSession deactivated (log removed)
         } catch {
            // AudioSession deactivation failed (log removed): \(error.localizedDescription)
         }
     }
    
    
    // MARK: - è§†é¢‘è¾“å…¥æ¥å£
    
    /// æ¥æ”¶ CVPixelBuffer (ä» StreamSessionViewModel ä¼ å…¥)
    func processVideoFrame(_ pixelBuffer: CVPixelBuffer) {
            print("[Video] processVideoFrame called - isRunning=\(isRunning) isConnected=\(isConnected) bufferWidth=\(CVPixelBufferGetWidth(pixelBuffer)) bufferHeight=\(CVPixelBufferGetHeight(pixelBuffer))")
             // ğŸ”’ ä¸¢å¸§ä¿æŠ¤ï¼šå¦‚æœç½‘ç»œè¿˜æ²¡å‘å®Œï¼Œæˆ–è€…æ¨¡å—æ²¡å¯åŠ¨ï¼Œç›´æ¥ä¸¢å¼ƒï¼Œä¸å å†…å­˜
             guard isRunning, isConnected else { return }
            
            processingQueue.async { [weak self] in
                guard let self = self else { return }
                
                // ğŸ’¡ ç›´æ¥æ“ä½œ Buffer è¿›è¡Œä¸­å¿ƒè£å‰ª (1280x640)
                // è¿™é‡Œæˆ‘ä»¬è°ƒç”¨ buffer(from: UIImage) é€»è¾‘çš„é€†è¿‡ç¨‹ï¼Œæˆ–è€…ä¸ºäº†å…¼å®¹æ€§
                // æš‚æ—¶æ²¿ç”¨ UIImage è£å‰ªï¼Œä½†å¿…é¡»åœ¨å¼‚æ­¥é˜Ÿåˆ—ä¸­å®Œæˆä»¥é˜² Code 22
                
                let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
                let context = CIContext()
                guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else { return }
                
                let originalImage = UIImage(cgImage: cgImage)
                let cropRect = CGRect(
                    x: (originalImage.size.width - 1280) / 2,
                    y: (originalImage.size.height - 640) / 2,
                    width: 1280,
                    height: 640
                )
                
                guard let croppedCG = cgImage.cropping(to: cropRect) else { return }
                let croppedUIImage = UIImage(cgImage: croppedCG)
                
                // ç¼©æ”¾åˆ° 360x180 (2:1 æ¯”ä¾‹)
                let targetSize = CGSize(width: 360, height: 180)
                UIGraphicsBeginImageContextWithOptions(targetSize, false, 1.0)
                croppedUIImage.draw(in: CGRect(origin: .zero, size: targetSize))
                let finalImage = UIGraphicsGetImageFromCurrentImageContext()
                UIGraphicsEndImageContext()
                
                if let processed = finalImage, let buffer = self.buffer(from: processed) {
                    print("[Video] Cropped and resized frame ready for encoding.")
                    self.encodeAndSend(buffer)
                } else {
                    print("[Video] Failed to crop/resize frame.")
                }
            }
        }
    // ç¡®ä¿è¿™ä¸ªæ–¹æ³•åœ¨ EmotionAnalysisManager ç±»çš„å¤§æ‹¬å·å†…
    private func sendVideoData(_ data: Data, isKeyframe: Bool) {
//        print("[Video] Sending video data: size=\(data.count), frame_index=\(frameIndex)")
        let payload: [String: Any] = [
            "timestamp": ISO8601DateFormatter().string(from: Date()),
            "frame_index": frameIndex,
            "codec": "H264",
            "width": 360,
            "height": 180,
            "is_keyframe": isKeyframe,
            "data": data.base64EncodedString(),
            "size": data.count
        ]
         self.sendMessage(type: "video", payload: payload, isPriority: true)
        frameIndex += 1
    }
    
    private func encodeAndSend(_ pixelBuffer: CVPixelBuffer) {
        let w = Int32(CVPixelBufferGetWidth(pixelBuffer))
        let h = Int32(CVPixelBufferGetHeight(pixelBuffer))
        
        // å¦‚æœç¼–ç å™¨å°šæœªåˆ›å»ºï¼Œæˆ–è€…åˆ†è¾¨ç‡å‘ç”Ÿå˜åŒ–ï¼Œåˆ™é‡æ–°åˆ›å»º
        if videoEncoder == nil || videoEncoder?.width != w || videoEncoder?.height != h {
            print("[Video] ğŸš€ åˆå§‹åŒ– H264 ç¼–ç å™¨: \(w)x\(h)")
            let newEncoder = H264Encoder()
            newEncoder.configure(width: w, height: h)
            
            // âœ… æ ¸å¿ƒä¿®å¤ï¼šæŒ‚è½½å›è°ƒï¼Œå°†ç¼–ç åçš„ Data é€å¾€ WebSocket
            newEncoder.callback = { [weak self] (data, isKeyframe) in
                self?.sendVideoData(data, isKeyframe: isKeyframe)
            }
            
            self.videoEncoder = newEncoder
        }
        
        // å–‚å…¥åƒç´ æ•°æ®è¿›è¡Œç¼–ç 
        videoEncoder?.encode(pixelBuffer)
    }
    // swift
    private func checkMicrophonePermissionAndStartAudio() {
        print("[Audio] ğŸ“‹ æ£€æŸ¥éº¦å…‹é£æƒé™...")
        if #available(iOS 17.0, *) {
            // iOS 17+ï¼šä½¿ç”¨æ–° API
            AVAudioApplication.requestRecordPermission { [weak self] granted in
                DispatchQueue.main.async {
                    if granted {
                        print("[Audio] âœ… éº¦å…‹é£æƒé™å·²è·å¾— (iOS 17+)")
                        self?.configureAudioSession()
                        self?.startAudioCapture()
                    } else {
                        print("[Audio] âŒ éº¦å…‹é£æƒé™è¢«æ‹’ç» (iOS 17+) - è¯·åœ¨è®¾ç½®ä¸­å¯ç”¨")
                    }
                }
            }
        } else {
            // iOS 16 åŠä»¥ä¸‹ï¼šä½¿ç”¨æ—§ API
            AVAudioSession.sharedInstance().requestRecordPermission { [weak self] granted in
                DispatchQueue.main.async {
                    if granted {
                        print("[Audio] âœ… éº¦å…‹é£æƒé™å·²è·å¾— (iOS 16-)")
                        self?.configureAudioSession()
                        self?.startAudioCapture()
                    } else {
                        print("[Audio] âŒ éº¦å…‹é£æƒé™è¢«æ‹’ç» (iOS 16-) - è¯·åœ¨è®¾ç½®ä¸­å¯ç”¨")
                    }
                }
            }
        }
    }
    func startAudioCapture() {
        print("[Audio] ğŸ™ï¸ å¯åŠ¨éŸ³é¢‘æ•è·...")
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            // âœ… æ”¹è¿›ï¼šæå‰åˆå§‹åŒ–å’ŒéªŒè¯ converter
            audioConverter = AVAudioConverter(from: recordingFormat, to: aacFormat)
            guard let audioConverter = audioConverter else {
                print("[Audio] âŒ æ— æ³•åˆ›å»º AAC è½¬æ¢å™¨")
                return
            }
            audioConverter.bitRate = 64000
            print("[Audio] âœ… AAC è½¬æ¢å™¨å·²åˆ›å»º: \(recordingFormat.sampleRate)Hz â†’ \(aacFormat.sampleRate)Hz")
        
        inputNode.removeTap(onBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 4096, format: recordingFormat) { [weak self] (buffer, time) in
            guard let self = self else { return }
            
            // âœ… æ”¹è¿›ï¼šåˆ†åˆ«æ£€æŸ¥æ¯ä¸ªæ¡ä»¶ï¼Œä¾¿äºè°ƒè¯•
            guard self.isRunning else { 
                // é™é»˜è¿”å›ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ
                return 
            }
            guard self.isConnected else { 
                // é™é»˜è¿”å›ï¼Œç­‰å¾…è¿æ¥
                return 
            }
            guard self.isAudioEnabled else { 
                // é™é»˜è¿”å›ï¼Œç­‰å¾…åç«¯å‡†å¤‡å¥½
                return 
            }
            guard let converter = self.audioConverter else { 
                print("[Audio] âš ï¸ è½¬æ¢å™¨ä¸º nil")
                return 
            }
            
            let aacBuffer = AVAudioCompressedBuffer(format: self.aacFormat, packetCapacity: 32, maximumPacketSize: converter.maximumOutputPacketSize)

            var error: NSError?
            let inputBlock: AVAudioConverterInputBlock = { _, outStatus in
                outStatus.pointee = .haveData
                return buffer
            }
            let status = converter.convert(to: aacBuffer, error: &error, withInputFrom: inputBlock)
            // âœ… æ”¹ä¸ºæ£€æŸ¥ status
            guard status == .haveData, aacBuffer.packetCount > 0 else {
                print("[Audio] AAC ç¼–ç å¤±è´¥æˆ–æ— æ•°æ®: status=\(status), packetCount=\(aacBuffer.packetCount)")
                return
            }

            // Prefer per-packet ADTS framing if packet descriptions are provided
            var outData = Data()
            let sampleRate = Int(self.aacFormat.sampleRate)
            let channelCount = UInt8(self.aacFormat.channelCount)

            if let packetDescPtr = aacBuffer.packetDescriptions {
                let packetDescBuffer = UnsafeBufferPointer(start: packetDescPtr, count: Int(aacBuffer.packetCount))
                for desc in packetDescBuffer {
                    let start = Int(desc.mStartOffset)
                    let size = Int(desc.mDataByteSize)
                    if size <= 0 { continue }
                    let packetBytes = Data(bytes: aacBuffer.data.advanced(by: start), count: size)
                    let header = self.adtsHeader(aacLength: size, sampleRate: sampleRate, channels: channelCount)
                    outData.append(header)
                    outData.append(packetBytes)
                }
            } else {
                // Fallback: treat whole buffer as a single AAC packet
                let aacData = Data(bytes: aacBuffer.data, count: Int(aacBuffer.byteLength))
                let header = self.adtsHeader(aacLength: aacData.count, sampleRate: sampleRate, channels: channelCount)
                outData.append(header)
                outData.append(aacData)
            }

            let aacPayload: [String: Any] = [
                "timestamp": ISO8601DateFormatter().string(from: Date()),
                "chunk_index": self.audioChunkIndex,
                "codec": "AAC",
                "sample_rate": sampleRate,
                "channels": Int(channelCount),
                "data": outData.base64EncodedString(),
                "size": outData.count
            ]
            wsSendQueue.async {
                print("[Audio] ğŸ“¤ å‘é€ AAC chunk: index=\(self.audioChunkIndex) size=\(outData.count) bytes")
                self.sendMessage(type: "audio", payload: aacPayload)
                }
            self.audioChunkIndex += 1
         }
         do {
             try audioEngine.start()
             print("[Audio] âœ… éŸ³é¢‘å¼•æ“å·²å¯åŠ¨ï¼Œå¼€å§‹æ•è·")
         } catch {
             print("[Audio] âŒ éŸ³é¢‘å¼•æ“å¯åŠ¨å¤±è´¥: \(error.localizedDescription)")
         }
     }
        
    // MARK: - éŸ³é¢‘åœæ­¢è¾“å…¥æ¥å£
    func stopAudioCapture() {
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
    }

        // MARK: - é€šä¿¡ä¸ä¿æ´» (åŸºäº Payload æ ‡å‡†)

        private func sendTriggerRequest() {
            let reqId = "req-" + UUID().uuidString
            currentRequestId = reqId
            let payload: [String: Any] = [
                "request_id": reqId,
                "user_id": userId,
                "messages": [["role": "user", "content": "s"]], // âœ… æ¨¡æ‹ŸæŒ‰é”® 's' æ¿€æ´»åç«¯
                "prep_data": [
                    "user_prompt": ["scene": "Live", "intention": "æƒ…ç»ªåˆ†æ", "analysis": "è¾“å‡ºå»ºè®®"]
                ],
                "snapshot_window_sec": 6,
                "is_last": false
            ]
            isAudioEnabled = true
            sendMessage(type: "text", payload: payload)
        }

        private func startMaintenanceHeartbeat() {
            heartbeatTimer?.invalidate()
            heartbeatTimer = Timer.scheduledTimer(withTimeInterval: 5.0, repeats: true) { [weak self] _ in
                guard let self = self, self.isConnected else { return }
                let payload: [String: Any] = [
                    "timestamp": ISO8601DateFormatter().string(from: Date()),
                    "presence_status": 1
                ]
                self.sendMessage(type: "vital", payload: payload) // âœ… ä½¿ç”¨ vital åŒ…ä¿æ´»
            }
        }
    
    
    // MARK: - WebSocket é€»è¾‘
    
    private func connectWebSocket() {
        webSocketTask = session.webSocketTask(with: wsURL)
        webSocketTask?.resume()
        
        // âœ… è¿™é‡Œç›´æ¥è®¤ä¸ºã€Œä¼ è¾“é€šé“å·²å»ºç«‹ã€ï¼Œå…è®¸å…ˆå‘èµ·é¦–åŒ…/æŒ‡ä»¤
        // å¦‚æœæœåŠ¡ç«¯è¿˜æœ‰ã€Œä¸šåŠ¡å°±ç»ªã€æ¦‚å¿µï¼Œå¯ä»¥å¦å¤–åŠ ä¸šåŠ¡å±‚çŠ¶æ€ï¼Œè€Œä¸æ˜¯å¡åœ¨ç‰©ç†è¿æ¥ä¸Š
        DispatchQueue.main.async {
            if !self.isConnected {
                self.isConnected = true
            }
        }
        
        print("[Emotion] WebSocket è¿æ¥å·²å‘èµ· (socket resumed)...")
        
        // å¼€å§‹æ¥æ”¶æ¶ˆæ¯
        receiveMessage()
        
        // ç¨åå°è¯•åˆ·æ–°é˜Ÿåˆ—å¹¶å‘é€åˆå§‹åŒ…ï¼ˆä»å¯ä¿ç•™çŸ­å»¶è¿Ÿï¼‰
        DispatchQueue.global().asyncAfter(deadline: .now() + 0.5) {
            self.flushOutgoingQueue()
            self.sendInitialText()
        }
    }
    final class AudioBatcher {

        // ===== é…ç½®å‚æ•° =====
        private let targetBatchBytes = 3200    // â‰ˆ 100ms @ 16kHz mono PCM16
        private let maxQueueBytes = 64 * 1024   // é˜²æ­¢æ— é™å †ç§¯
        private let sendInterval: TimeInterval = 0.05 // 20 FPS

        // ===== ä¾èµ– =====
        private let send: (Data) -> Void

        // ===== å†…éƒ¨çŠ¶æ€ =====
        private var buffer = Data()
        private let queue = DispatchQueue(label: "audio.batching.queue")
        private var timer: DispatchSourceTimer?

        init(send: @escaping (Data) -> Void) {
            self.send = send
            startTimer()
        }

        // ===== å¤–éƒ¨å…¥å£ =====
        func append(_ data: Data) {
            queue.async {
                // é™åˆ¶æ€»ç¼“å­˜
                if self.buffer.count + data.count > self.maxQueueBytes {
                    print("âš ï¸ [AudioBatcher] buffer overflow, drop old audio")
                    self.buffer.removeFirst(min(self.buffer.count, data.count))
                }
                self.buffer.append(data)
            }
        }

        // ===== å®šæ—¶ flush =====
        private func startTimer() {
            timer = DispatchSource.makeTimerSource(queue: queue)
            timer?.schedule(deadline: .now() + sendInterval, repeating: sendInterval)
            timer?.setEventHandler { [weak self] in
                self?.flush()
            }
            timer?.resume()
        }

        private func flush() {
            guard buffer.count >= targetBatchBytes else { return }

            let chunk = buffer.prefix(targetBatchBytes)
            buffer.removeFirst(targetBatchBytes)

            send(chunk)
        }

        deinit {
            timer?.cancel()
        }
    }

    // swiftï¼ˆä¿ç•™è¿™ä¸ªç‰ˆæœ¬ï¼Œåˆ é™¤é‡å¤çš„ï¼‰
    private func receiveMessage() {
        webSocketTask?.receive { [weak self] result in
            guard let self = self else { return }
            
            switch result {
            case .success(let message):
                switch message {
                case .string(let text):
                    self.handleMessage(text)
                case .data(let data):
                    self.handleMessageData(data)
                @unknown default:
                    break
                }
                self.flushOutgoingQueue()

                if self.isConnected {
                    self.receiveMessage()  // é€’å½’ç»§ç»­ç›‘å¬
                }
                
            case .failure(let error):
                print("[Emotion] WebSocket Error: \(error)")
                self.disconnectWebSocket()
            }
        }
    }

    
    private func disconnectWebSocket() {
            print("[Emotion] WebSocket æ–­å¼€è¿æ¥")
            webSocketTask?.cancel(with: .goingAway, reason: nil)
            webSocketTask = nil
            
            DispatchQueue.main.async {
                self.isConnected = false
                
                // ğŸ”¥ğŸ”¥ æ–°å¢ï¼šé”€æ¯ç¼–ç å™¨
                // è¿™æ ·ä¸‹æ¬¡è¿æ¥æ—¶ä¼šé‡æ–°åˆ›å»ºï¼Œä»è€Œå¼ºåˆ¶å‘é€ SPS/PPS å’Œ å…³é”®å¸§
                self.videoEncoder = nil
                print("[Emotion] ç¼–ç å™¨å·²é‡ç½®ï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡ä¼šè¯")
            }
        }
    
    

    
    // Thread-safe sender: queue when not connected, flush when possible
    // swift
    private let wsSendQueue = DispatchQueue(label: "ws.send.queue")
    // MARK: - ä¼˜åŒ–åçš„å‘é€ç³»ç»Ÿ
    private func sendMessage(type: String, payload: Any, isPriority: Bool = false) {
        guard isRunning else { return }

        var finalPayload = payload
        // 1. å¿«é€Ÿæ•°æ®æ¸…ç†
        if var dict = payload as? [String: Any] {
            if let dataStr = dict["data"] as? String {
                dict["data"] = dataStr.components(separatedBy: .whitespacesAndNewlines).joined()
            }
            if let requestId = currentRequestId, dict["request_id"] == nil {
                dict["request_id"] = requestId
            }
            finalPayload = dict
        }

        var msg: [String: Any] = ["message_type": type, "payload": finalPayload]
        if let requestId = currentRequestId {
            msg["request_id"] = requestId
        }
        guard let data = try? JSONSerialization.data(withJSONObject: msg),
              let json = String(data: data, encoding: .utf8) else { return }

        // 2. è§†é¢‘å¸§(H264)å’Œè§¦å‘æŒ‡ä»¤(Text)å¼ºåˆ¶è®¾ä¸ºé«˜ä¼˜å…ˆçº§
        let needsPriority = isPriority || (type == "video" || type == "text")

        outgoingQueueLock.async { [weak self] in
            guard let self = self else { return }
            
            if needsPriority {
                // é«˜ä¼˜å…ˆçº§æ’åˆ°é˜Ÿé¦–ï¼Œç¡®ä¿åç«¯åœ¨ 15s çª—å£å†…ä¼˜å…ˆçœ‹åˆ°è„¸
                self.outgoingQueue.insert(json, at: 0)
            } else {
                // æ™®é€šåª’ä½“æ•°æ®ï¼Œå¦‚æœç§¯å‹ä¸¥é‡åˆ™ç›´æ¥ä¸¢å¼ƒæ—§åŒ…ï¼ˆé˜²æ­¢ Code 57ï¼‰
                if self.outgoingQueue.count > 15 {
                    self.outgoingQueue.removeFirst(self.outgoingQueue.count - 10)
                    print("âš ï¸ [Queue] æ‹¥å¡ä¸¢åŒ…ï¼šæ¸…ç†æ—§éŸ³é¢‘/ä½“å¾æ•°æ®")
                }
                self.outgoingQueue.append(json)
            }
            
            // 3. ç«‹å³è§¦å‘åŒæ­¥å‘é€é€»è¾‘
            self.flushOutgoingQueue()
        }
    }

    private func flushOutgoingQueue() {
        // æå‡ QOS è§£å†³çº¿ç¨‹ä¼˜å…ˆçº§åè½¬
        outgoingWorkerQueue.async { [weak self] in
            guard let self = self, self.isConnected, let task = self.webSocketTask else { return }
            
            while true {
                var msgToSend: String?
                self.outgoingQueueLock.sync {
                    if !self.outgoingQueue.isEmpty {
                        msgToSend = self.outgoingQueue.removeFirst()
                    }
                }

                guard let msg = msgToSend else { break }

                // 4. æ¨¡æ‹ŸåŒæ­¥å‘é€ï¼šä½¿ç”¨ä¿¡å·é‡é˜»å¡ worker çº¿ç¨‹ç›´åˆ°è¯¥åŒ…å‘å‡º
                let semaphore = DispatchSemaphore(value: 0)
                task.send(.string(msg)) { error in
                    if let err = error {
                        print("âŒ [Flush] å‘é€å¤±è´¥: \(err.localizedDescription)")
                        // ä»…æŒ‡ä»¤é‡å…¥é˜Ÿï¼ŒéŸ³è§†é¢‘ä¸¢å¼ƒé˜²æ­¢æ­»å¾ªç¯
                    }
                    semaphore.signal()
                }
                
                // ç­‰å¾…å½“å‰åŒ…å‘å®Œå†å‘ä¸‹ä¸€ä¸ªï¼Œç¡®ä¿é¡ºåºå’Œä½å»¶è¿Ÿ
                let result = semaphore.wait(timeout: .now() + 1.0)
                if result == .timedOut { break }
            }
        }
    }


    private func enqueueOutgoing(_ json: String) {
        outgoingQueueLock.async {
            self.outgoingQueue.append(json)
            if self.outgoingQueue.count > 50 {
                        os_log("âš ï¸ [Network_Congestion] Queue size reached %d", log: performanceLog, type: .error, self.outgoingQueue.count)
                    }
            if self.outgoingQueue.count > 500 { self.outgoingQueue.removeFirst(self.outgoingQueue.count - 500) }
            print("[Queue] enqueued (size=\(self.outgoingQueue.count))")
        }
    }


    private func reconnectWebSocketIfNeeded() {
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            if self.webSocketTask == nil || !self.isConnected {
                print("[Reconnect] attempting reconnect")
                self.connectWebSocket()
            }
        }
    }
    

    
    
    // MARK: - è§†é¢‘è¾“å…¥æ¥å£
    
    // åœ¨ EmotionAnalysisManager.swift ä¸­æ‰¾åˆ°è¿™ä¸ªæ–¹æ³•å¹¶æ›¿æ¢

    private func handleMessage(_ text: String) {
        print("ğŸ“¥ [Raw Data from Server]: \(text)")
        guard let data = text.data(using: .utf8) else { return }
        handleMessageData(data)
    }

    private func handleMessageData(_ data: Data) {
        guard let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any] else { return }
        
        let messageType = json["message_type"] as? String
        
        if messageType == "chunk",
           let payload = json["payload"] as? [String: Any] {
            
            // è§£æ emotion_result (è¿™æ˜¯ä½  client.py é‡Œå®šä¹‰çš„ç»“æ„)
            if let result = payload["emotion_result"] as? [String: Any] {
                let emotionBlock = result["emotion"] as? [String: Any]
                let scores = emotionBlock?["emotion"] as? [String: Double]
                let analysisText = emotionBlock?["analysis"] as? String
                let intention = result["intention"] as? [String: Any]
                let detected = intention?["detected_intentions"] as? [[String: Any]]
                let firstIntention = detected?.first
                let reasoning = firstIntention?["reasoning"] as? String
                let recommendedContent = intention?["recommended_content"] as? [String: Any]
                let suggestion = recommendedContent?["suggestion"] as? String

                DispatchQueue.main.async {
                    // 1. æå–è¯¦ç»†æƒ…ç»ªåˆ†æ•° (ç”¨äºç”»å›¾)
                    if let scores = scores {
                        self.emotionScores = scores
                        // æ‰¾å‡ºåˆ†æ•°æœ€é«˜çš„æƒ…ç»ªä½œä¸ºä¸»å¯¼
                        self.dominantEmotion = scores.max { a, b in a.value < b.value }?.key ?? "Neutral"
                    }
                    
                    // 2. æå–å»ºè®® (Analysis å­—æ®µåŒ…å«å»ºè®®)
                    if let analysisText = analysisText {
                        self.emotionResult = analysisText // ç®€ç•¥ç‰ˆ
                        self.aiAdvice = analysisText // è¯¦ç»†å»ºè®®
                    }

                    // 2.1 å¦‚æœåç«¯æœ‰æ›´æ˜ç¡®çš„å»ºè®®ï¼Œä¼˜å…ˆè¦†ç›–
                    if let suggestion = suggestion, !suggestion.isEmpty {
                        self.aiAdvice = suggestion
                    }
                    
                    // 3. æå–æ·±åº¦æ¨ç† (Intention -> Reasoning)
                    if let reasoning = reasoning {
                        self.aiReasoning = reasoning
                    }
                }
                
                print("ğŸ§  [æ›´æ–°æ•°æ®] ä¸»å¯¼æƒ…ç»ª: \(self.dominantEmotion)")
            }
        }
    }
    
    // MARK: - å‘é€é€»è¾‘
    

    
    private func startFakeVitalSigns() {
        Timer.scheduledTimer(withTimeInterval: 3.0, repeats: true) { [weak self] timer in
            guard let self = self, self.isRunning, self.isConnected else { return }
            
            // âœ… ä¼˜åŒ–ï¼šå¢å¤§é—´éš”ä» 2s åˆ° 3sï¼Œå‡å°‘å‘é€é¢‘ç‡ä»¥ç¼“è§£æ‹¥å¡
            let payload: [String: Any] = [
                "timestamp": ISO8601DateFormatter().string(from: Date()),
                "heart_rate": Double.random(in: 70...90),
                "breath_rate": Double.random(in: 12...20),
                "breath_amp": Double.random(in: 0.5...1.0),
                "conf": 0.95,
                "init_stat": 1,
                "presence_status": 1
            ]
            self.sendMessage(type: "vital", payload: payload)
        }
    }
    
    private func sendInitialText() {
         let prepData: [String: Any] = [
             "user_prompt": [
                 "scene": "Ray-Bançœ¼é•œç¬¬ä¸€è§†è§’",
                 "intention": "åˆ†æç”¨æˆ·æ‰€å¤„ç¯å¢ƒå‹åŠ›",
                 "analysis": "è¾“å‡ºæƒ…ç»ªæ ‡ç­¾ä¸å»ºè®®"
             ]
         ]
         let payload: [String: Any] = [
             "user_id": userId,
             "messages": [["role": "user", "content": "Start Analysis"]],
             "prep_data": prepData,
             "snapshot_window_sec": 15,
             "is_last": false
         ]
         sendMessage(type: "text", payload: payload)
     }
    
    // Public debug helper to send a synthetic audio payload from the Xcode console.
    // Usage (Xcode debug console):
    // expr -l Swift -- EmotionAnalysisManager.shared.debugSendTestAudio()
//    @objc public func debugSendTestAudio() {
//        let raw = "test-audio"
//        let b64 = raw.data(using: .utf8)!.base64EncodedString()
//        let payload: [String: Any] = [
//            "timestamp": ISO8601DateFormatter().string(from: Date()),
//            "chunk_index": self.audioChunkIndex,
//            "codec": "AAC",
//            "sample_rate": 24000,
//            "channels": 1,
//            "data": b64,
//            "size": b64.count
//        ]
//        print("[Debug] Sending synthetic audio payload (chunk_index=\(self.audioChunkIndex))")
//        self.sendMessage(type: "audio", payload: payload)
//        self.audioChunkIndex += 1
//    }
    // MARK: - ä¸‡èƒ½é€‚é…æ¥å£ (UIImage -> H.264)
    
    /// å½“æ— æ³•è·å–åŸå§‹ buffer æ—¶ï¼Œä½¿ç”¨æ­¤æ–¹æ³•ä¼ å…¥ UIImage
    // åœ¨ EmotionAnalysisManager.swift ä¸­

    func processUIImage(_ image: UIImage) {
        let signpostID = OSSignpostID(log: performanceLog)
                os_signpost(.begin, log: performanceLog, name: "ImageProcessing", signpostID: signpostID)
        guard isRunning, isConnected else { return }

        processingQueue.async { [weak self] in
            guard let self = self else { return }

            // 1. è®¡ç®—å±…ä¸­è£å‰ªåŒºåŸŸ (ä¿æŒ 2:1 æ¯”ä¾‹)
            let originalSize = image.size
            let targetAspect: CGFloat = 2.0
            var cropWidth = originalSize.width
            var cropHeight = originalSize.height

            if originalSize.width / originalSize.height > targetAspect {
                cropWidth = originalSize.height * targetAspect
            } else {
                cropHeight = originalSize.width / targetAspect
            }

            let cropRect = CGRect(
                x: (originalSize.width - cropWidth) / 2,
                y: (originalSize.height - cropHeight) / 2,
                width: cropWidth,
                height: cropHeight
            )

            // 2. æ‰§è¡Œè£å‰ªä¸ç¼©æ”¾
            guard let cgImage = image.cgImage?.cropping(to: cropRect) else { return }
            let targetSize = CGSize(width: 360, height: 180)
            
            UIGraphicsBeginImageContextWithOptions(targetSize, false, 1.0)
            UIImage(cgImage: cgImage).draw(in: CGRect(origin: .zero, size: targetSize))
            let finalImage = UIGraphicsGetImageFromCurrentImageContext()
            UIGraphicsEndImageContext()

            // 3. è½¬æ¢å¹¶ç›´æ¥é€å»ç¼–ç 
            if let processed = finalImage, let buffer = self.buffer(from: processed) {
                // ç›´æ¥è·³è¿‡åŸæ¥çš„ processVideoFrameï¼Œç›´æ¥å»ç¼–ç 
                self.encodeAndSend(buffer)
            }
            
        }
    }
    
    // è¾…åŠ©å‡½æ•°ï¼šUIImage è½¬ CVPixelBuffer
     private func buffer(from image: UIImage) -> CVPixelBuffer? {
        // Create a CVPixelBuffer with NV12 pixel format which is supported by VideoToolbox hardware encoder
        let width = Int(image.size.width)
        let height = Int(image.size.height)

        var pixelBuffer: CVPixelBuffer?
        let attrs = [kCVPixelBufferIOSurfacePropertiesKey: [:]] as CFDictionary
        let status = CVPixelBufferCreate(kCFAllocatorDefault, width, height, kCVPixelFormatType_420YpCbCr8BiPlanarFullRange, attrs, &pixelBuffer)
        guard status == kCVReturnSuccess, let pb = pixelBuffer else { return nil }

        CVPixelBufferLockBaseAddress(pb, CVPixelBufferLockFlags(rawValue: 0))
        defer { CVPixelBufferUnlockBaseAddress(pb, CVPixelBufferLockFlags(rawValue: 0)) }

        // Render UIImage -> CIImage -> CVPixelBuffer (NV12)
        let ciContext = CIContext(options: nil)
        var ciImage: CIImage?
        if let cg = image.cgImage {
            ciImage = CIImage(cgImage: cg)
        } else if let ci = image.ciImage {
            ciImage = ci
        } else {
            return nil
        }

        // Scale CIImage to target pixel buffer size if needed
        let imgExtent = ciImage!.extent
        if Int(imgExtent.width) != width || Int(imgExtent.height) != height {
            let scaleX = CGFloat(width) / imgExtent.width
            let scaleY = CGFloat(height) / imgExtent.height
            let transform = CGAffineTransform(scaleX: scaleX, y: scaleY)
            ciImage = ciImage!.transformed(by: transform)
        }

        ciContext.render(ciImage!, to: pb)
        return pb
    }
    // MARK: - è¾…åŠ©ä»»åŠ¡ (å‘é€è§¦å‘è¯·æ±‚ä¸ä¿æ´»)
    /// æ­¥éª¤ B: å¯åŠ¨ä¿æ´»å¿ƒè·³ï¼Œé˜²æ­¢åç«¯ 10 ç§’æ€è¿›ç¨‹
    func startTriggerRequestTimer() {
        triggerRequestTimer?.invalidate()
        triggerRequestTimer = Timer.scheduledTimer(withTimeInterval: 15.0, repeats: true) { [weak self] _ in
            guard let self = self, self.isConnected else { return }
            self.sendTriggerRequest()
        }
    }
    
    func stopTriggerRequestTimer() {
        triggerRequestTimer?.invalidate()
        triggerRequestTimer = nil
    }
}



// MARK: - H.264 ç¡¬ä»¶ç¼–ç å™¨ (ç»ˆæä¿®å¤ç‰ˆï¼šAVCC è½¬ Annex B + è‡ªåŠ¨æ’å…¥ SPS/PPS)
// âš ï¸ å¿…é¡»ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬ï¼Œå¦åˆ™åç«¯æ— æ³•è§£ç ï¼Œä¼šç›´æ¥æ–­å¼€è¿æ¥ (TCP Reset)ï¼

class H264Encoder {
    private var session: VTCompressionSession?
    var width: Int32 = 0
    var height: Int32 = 0
    var callback: ((Data, Bool) -> Void)? // å›è°ƒ
    
    func configure(width: Int32, height: Int32) {
        self.width = width
        self.height = height

        // âœ… å¿…é¡»æŒ‡å®šåƒç´ æ ¼å¼ï¼Œå¦åˆ™ VTCompressionSessionCreate ä¼šå¤±è´¥
        let imageBufferAttributes: [String: Any] = [
            kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange,
            kCVPixelBufferWidthKey as String: width,
            kCVPixelBufferHeightKey as String: height
        ]

        var session: VTCompressionSession?
        let status = VTCompressionSessionCreate(
            allocator: kCFAllocatorDefault,
            width: width,
            height: height,
            codecType: kCMVideoCodecType_H264,
            encoderSpecification: nil,
            imageBufferAttributes: imageBufferAttributes as CFDictionary,  // âœ… å…³é”®ä¿®å¤
            compressedDataAllocator: nil,
            outputCallback: { (refCon, _, status, _, sampleBuffer) in
                guard status == noErr, let sampleBuffer = sampleBuffer else { return }
                let encoder = Unmanaged<H264Encoder>.fromOpaque(refCon!).takeUnretainedValue()
                encoder.handleEncodedFrame(sampleBuffer)
            },
            refcon: UnsafeMutableRawPointer(Unmanaged.passUnretained(self).toOpaque()),
            compressionSessionOut: &session
        )

        guard status == noErr, let session = session else {
            print("âŒ H264: åˆ›å»ºä¼šè¯å¤±è´¥ - status=\(status)")
            self.session = nil
            return
        }

        self.session = session
        VTSessionSetProperty(session, key: kVTCompressionPropertyKey_RealTime, value: kCFBooleanTrue)
        VTSessionSetProperty(session, key: kVTCompressionPropertyKey_ProfileLevel, value: kVTProfileLevel_H264_Baseline_AutoLevel)
        VTSessionSetProperty(session, key: kVTCompressionPropertyKey_AllowFrameReordering, value: kCFBooleanFalse)
        VTSessionSetProperty(session, key: kVTCompressionPropertyKey_MaxKeyFrameInterval, value: 30 as CFNumber)
        // âœ… ä¼˜åŒ–ï¼šè®¾ç½®ç ç‡é™åˆ¶åˆ° 500kbps ä»¥å‡å°‘ç½‘ç»œå‹åŠ›
        VTSessionSetProperty(session, key: kVTCompressionPropertyKey_AverageBitRate, value: 500000 as CFNumber)
        VTCompressionSessionPrepareToEncodeFrames(session)
        print("âœ… H264: ç¼–ç å™¨åˆå§‹åŒ–æˆåŠŸ \(width)x\(height)")
    }
    
    func encode(_ pixelBuffer: CVPixelBuffer) {
//        print("[H264Encoder] encode called - sessionInitialized=\(session != nil), bufferWidth=\(CVPixelBufferGetWidth(pixelBuffer)), bufferHeight=\(CVPixelBufferGetHeight(pixelBuffer))")
        guard let session = session else { print("[H264Encoder] encode aborted - session nil"); return }
         let time = CMTime(seconds: CACurrentMediaTime(), preferredTimescale: 1000)
         
         let status = VTCompressionSessionEncodeFrame(
             session,
             imageBuffer: pixelBuffer,
             presentationTimeStamp: time,
             duration: .invalid,
             frameProperties: nil,
             sourceFrameRefcon: nil,
             infoFlagsOut: nil
         )
         
         if status != noErr {
             print("âŒ H264: ç¼–ç å¸§å¤±è´¥ \(status)")
         }
     }
    // åœ¨ H264Encoder ç±»ä¸­

    private func handleEncodedFrame(_ sampleBuffer: CMSampleBuffer) {
        guard let dataBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) else { return }
        
        // 1. æ£€æŸ¥å…³é”®å¸§æ ‡è¯†
        let attachments = CMSampleBufferGetSampleAttachmentsArray(sampleBuffer, createIfNecessary: true) as? [[CFString: Any]]
        let isKeyframe = !(attachments?.first?[kCMSampleAttachmentKey_NotSync] as? Bool ?? false)
        
        var elementaryStream = Data()
        
        // 2. å…³é”®å¸§å¿…é¡»å…ˆå†™å…¥å‚æ•°é›† (SPS/PPS)
        if isKeyframe {
            if let description = CMSampleBufferGetFormatDescription(sampleBuffer) {
                var paramCount: Int = 0
                // è·å–å‚æ•°é›†æ•°é‡
                CMVideoFormatDescriptionGetH264ParameterSetAtIndex(description, parameterSetIndex: 0, parameterSetPointerOut: nil, parameterSetSizeOut: nil, parameterSetCountOut: &paramCount, nalUnitHeaderLengthOut: nil)
                
                for i in 0..<paramCount {
                    var parameterSetPointer: UnsafePointer<UInt8>?
                    var parameterSetLength: Int = 0
                    CMVideoFormatDescriptionGetH264ParameterSetAtIndex(description, parameterSetIndex: i, parameterSetPointerOut: &parameterSetPointer, parameterSetSizeOut: &parameterSetLength, parameterSetCountOut: nil, nalUnitHeaderLengthOut: nil)
                    
                    if let pointer = parameterSetPointer {
                        // æ ‡å‡† Annex B èµ·å§‹ç 
                        elementaryStream.append(contentsOf: [0x00, 0x00, 0x00, 0x01])
                        elementaryStream.append(pointer, count: parameterSetLength)
                    }
                }
            }
        }
        
        // 3. è·å– AVCC å¤´çš„é•¿åº¦ (iOS é»˜è®¤ä¸º 4 å­—èŠ‚)
        var avccHeaderLength: Int32 = 4
        if let description = CMSampleBufferGetFormatDescription(sampleBuffer) {
            CMVideoFormatDescriptionGetH264ParameterSetAtIndex(description, parameterSetIndex: 0, parameterSetPointerOut: nil, parameterSetSizeOut: nil, parameterSetCountOut: nil, nalUnitHeaderLengthOut: &avccHeaderLength)
        }
        let headerLen = Int(avccHeaderLength)

        // 4. å°† AVCC åŒ…è£…çš„ NALU è½¬æ¢ä¸º Annex B
        var lengthAtOffset: Int = 0
        var totalLength: Int = 0
        var dataPointer: UnsafeMutablePointer<Int8>?
        
        if CMBlockBufferGetDataPointer(dataBuffer, atOffset: 0, lengthAtOffsetOut: &lengthAtOffset, totalLengthOut: &totalLength, dataPointerOut: &dataPointer) == noErr {
            var bufferOffset = 0
            while bufferOffset < totalLength - headerLen {
                var nalUnitLength: UInt32 = 0
                // è¯»å–å½“å‰ NALU çš„é•¿åº¦
                memcpy(&nalUnitLength, dataPointer! + bufferOffset, headerLen)
                // å¤§ç«¯è½¬æœ¬æœºåº (å…³é”®ä¸€æ­¥)
                nalUnitLength = CFSwapInt32BigToHost(nalUnitLength)
                
                // å†™å…¥èµ·å§‹ç 
                elementaryStream.append(contentsOf: [0x00, 0x00, 0x00, 0x01])
                
                // å†™å…¥çœŸæ­£çš„ NALU æ•°æ®
                let dataPtr = dataPointer! + bufferOffset + headerLen
                elementaryStream.append(Data(bytes: dataPtr, count: Int(nalUnitLength)))
                
                // ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª NALU
                bufferOffset += headerLen + Int(nalUnitLength)
            }
        }
        
        // 5. æ ¡éªŒå‘é€ï¼šå¦‚æœæ•°æ®å¤ªå°ï¼ˆæ¯”å¦‚åªæœ‰ 4 å­—èŠ‚èµ·å§‹ç ï¼‰ï¼Œåˆ™ä¸å‘é€
        if elementaryStream.count > 4 {
            callback?(elementaryStream, isKeyframe)
        }
    }
    
    

}
